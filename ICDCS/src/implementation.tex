\section{Implementation in P4 Hardware Switch}
\label{section:implementation}
As stated before, we have implemented HashFlow in bmv2, the software P4 switch, as well as in a hardware P4 switch which has the type of Wedge 100BF-32X\cite{noauthor_edgecore_nodate}. The code can be found at \cite{zhao_hashflow_2018}. Although both versions of the algorithm are implemented using P4$_{14}$, the grammar checking of the hardware switch is stricter than that of the software switch, and the implementation is heavily limited by the resource restrictions of the hardware. In this section, we will discuss the hardware implementation of HashFlow in detail.

The P4 program will be compiled into a pipeline, which consists of multiple stages. Multiple small match-action tables can be packed into a stage, and a large table may span multiple stages. The tables within the same stage can be executed in parallel, while the stages can only be executed serially and tables that are dependent on each other must be distributed among different stages. The compiler will analyze the dependency relationship of the tables and arrange the tables within the stages automatically. The amount of processing within a single stage is upper bounded, so the processing time of a single stage is limited. Moreover, to upper bound the processing delay within a single P4 switch, the number of stages that a switch can support is limited. Our switch can support 12 stages at most.
In our implementation, we identify a flow with the typical 5 tuples and store the flow records into 6 register arrays, i.e., one register array for source/destination IP address, protocol, source/destination port and packet count respectively. A register array can only be accessed using stateful ALUs, which can execute a simple program atomically. As an action of P4 cannot access more than one register array (through stateful ALUs), we implement 6 match-action tables to access a flow record. Since accesses of the IP addresses, protocol and ports are independent, and the operations imposed upon packet count are determined by the result from accessing the flow ID, the tables corresponding to the arrays of IP addresses, protocol and ports can be packed into a single stage, while the table corresponding to the packet count has to be put into another stage. We define an \emph{iteration} to be all the processing operations corresponding to a pipelined table or the ancillary table, so HashFlow contains $d+1$ iterations. In our implementation, the first iteration needs 2 stages, and each of the following iterations needs 4 stages, so HashFlow needs $4\times d + 2$ stages and only $d\le 2$ is allowed in our P4 switch. To support more pipelined tables, more resources or advanced techniques to refine the implementation are needed.

To allow the pipelining of the packets, multiple tables sharing the same register array in a pipeline can only access the resource exclusively, which means that only one table can access the resource when processing a packet. In Algorithm~\ref{alg: process_packet}, we may have to visit the hash table $d$ times when doing collision resolution, violating the access restriction. We can address the problem by splitting the hash table into $d$ small tables, so only the pipelined-tables scheme of main table is feasible in P4 hardware switch. However, when computing the index from a flow ID, the size of the index space must be the power of 2, so the table arrangement stated in Section~\ref{analysis}, i.e., decreasing the size of the pipelined tables in a factor of 0.7, is infeasible. We simply set the size of each table to be the same in our hardware implementation.

Another challenge in implementing HashFlow is that record promotion requires to revisit one of the pipelined tables, even if we have visited every table when doing collision resolution, thus violating the access restriction. Our solution is to resubmit the current packet and process it again when doing record promotion. By marking some metadata, we will be able to evict an existing flow record and set up a new record for this packet. Since more packets than that feed into the switch are processed when the resubmit primitive is used, the throughput of the switch will be degraded. In Section~\ref{subsec:throughput} we will evaluate the loss of throughput caused by resubmit primitive.

